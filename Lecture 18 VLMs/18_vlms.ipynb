{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b2774e9",
   "metadata": {},
   "source": [
    "# Vision-Language Models: Understanding How AI Sees and Comprehends\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Vision-Language Models (VLMs) are AI systems that can understand and reason about both visual and textual information simultaneously. Unlike traditional computer vision models that only classify or detect objects, VLMs can:\n",
    "- Answer questions about images\n",
    "- Generate detailed descriptions\n",
    "- Understand visual relationships and context\n",
    "- Perform zero-shot classification without training examples\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this session, you will be able to:\n",
    "\n",
    "1. **Explain** how contrastive learning creates shared embedding spaces between vision and text\n",
    "2. **Demonstrate** the dual-encoder architecture of CLIP and how it enables zero-shot transfer\n",
    "3. **Analyze** attention mechanisms in vision transformers and their role in visual understanding\n",
    "4. **Compare** different vision-language architectures (CLIP vs SmolVLM) and their trade-offs\n",
    "5. **Implement** practical applications using vision-language models\n",
    "6. **Evaluate** the impact of instruction tuning on model capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce9d5ff",
   "metadata": {},
   "source": [
    "## Setup and Requirements\n",
    "\n",
    "Let's start by setting up our environment with all necessary libraries. This cell installs required packages and pre-loads models to ensure smooth execution throughout the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfb19ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Users/jinming/Dropbox/learn-ml-by-building/venv_macbook/bin/python -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Users/jinming/Dropbox/learn-ml-by-building/venv_macbook/bin/python -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Users/jinming/Dropbox/learn-ml-by-building/venv_macbook/bin/python -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Users/jinming/Dropbox/learn-ml-by-building/venv_macbook/bin/python -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Users/jinming/Dropbox/learn-ml-by-building/venv_macbook/bin/python -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Users/jinming/Dropbox/learn-ml-by-building/venv_macbook/bin/python -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Users/jinming/Dropbox/learn-ml-by-building/venv_macbook/bin/python -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Users/jinming/Dropbox/learn-ml-by-building/venv_macbook/bin/python -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Users/jinming/Dropbox/learn-ml-by-building/venv_macbook/bin/python -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup complete!\n",
      "PyTorch version: 2.2.2\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = ['transformers', 'torch', 'torchvision', 'pillow', 'matplotlib', 'numpy', 'plotly', 'ipywidgets', 'scikit-learn']\n",
    "for package in packages:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', package])\n",
    "\n",
    "# Import core libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99abae2e",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: The Magic of Vision-Language Understanding\n",
    "\n",
    "### The Fundamental Challenge\n",
    "\n",
    "Before diving into architectures, let's understand why vision-language modeling is challenging:\n",
    "\n",
    "- **Images**: Dense grids of pixels (e.g., 224×224×3 = 150,528 continuous values)\n",
    "- **Text**: Variable-length sequences of discrete tokens\n",
    "- **Challenge**: How do we compare or relate these completely different data types?\n",
    "\n",
    "The breakthrough insight: Instead of trying to convert one to the other, we can map both to a shared embedding space where similar concepts are close together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbf2e360",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-19 17:09:52.757267: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIP model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d3ee30ef2ae40929c415a17d412d07e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load CLIP model\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading CLIP model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m clip_model \u001b[38;5;241m=\u001b[39m \u001b[43mCLIPModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mopenai/clip-vit-base-patch32\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m clip_processor \u001b[38;5;241m=\u001b[39m CLIPProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai/clip-vit-base-patch32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Load a sample image from URL\u001b[39;00m\n",
      "File \u001b[0;32m~/Dropbox/learn-ml-by-building/venv_macbook/lib/python3.9/site-packages/transformers/modeling_utils.py:277\u001b[0m, in \u001b[0;36mrestore_default_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m~/Dropbox/learn-ml-by-building/venv_macbook/lib/python3.9/site-packages/transformers/modeling_utils.py:5048\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   5038\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5039\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   5041\u001b[0m     (\n\u001b[1;32m   5042\u001b[0m         model,\n\u001b[1;32m   5043\u001b[0m         missing_keys,\n\u001b[1;32m   5044\u001b[0m         unexpected_keys,\n\u001b[1;32m   5045\u001b[0m         mismatched_keys,\n\u001b[1;32m   5046\u001b[0m         offload_index,\n\u001b[1;32m   5047\u001b[0m         error_msgs,\n\u001b[0;32m-> 5048\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5053\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5054\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5057\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5058\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5059\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5060\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5061\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5062\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5063\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5064\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   5065\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/Dropbox/learn-ml-by-building/venv_macbook/lib/python3.9/site-packages/transformers/modeling_utils.py:5316\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[0m\n\u001b[1;32m   5313\u001b[0m     original_checkpoint_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(state_dict\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m   5314\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5315\u001b[0m     original_checkpoint_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m-> 5316\u001b[0m         \u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m   5317\u001b[0m     )\n\u001b[1;32m   5319\u001b[0m \u001b[38;5;66;03m# Check if we are in a special state, i.e. loading from a state dict coming from a different architecture\u001b[39;00m\n\u001b[1;32m   5320\u001b[0m prefix \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mbase_model_prefix\n",
      "File \u001b[0;32m~/Dropbox/learn-ml-by-building/venv_macbook/lib/python3.9/site-packages/transformers/modeling_utils.py:508\u001b[0m, in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file, is_quantized, map_location, weights_only)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# Fallback to torch.load (if weights_only was explicitly False, do not check safety as this is known to be unsafe)\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[0;32m--> 508\u001b[0m     \u001b[43mcheck_torch_load_is_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m map_location \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Dropbox/learn-ml-by-building/venv_macbook/lib/python3.9/site-packages/transformers/utils/import_utils.py:1647\u001b[0m, in \u001b[0;36mcheck_torch_load_is_safe\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1645\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcheck_torch_load_is_safe\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1646\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_greater_or_equal(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.6\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1647\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1648\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDue to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1649\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1650\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhen loading files with safetensors.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1651\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1652\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel, AutoProcessor, AutoModelForVision2Seq\n",
    "\n",
    "# Load CLIP model\n",
    "print(\"Loading CLIP model...\")\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Load a sample image from URL\n",
    "def load_image_from_url(url):\n",
    "    \"\"\"Load an image from a URL with a robust fallback.\n",
    "\n",
    "    If the primary URL fails (e.g., 503 from a remote image service),\n",
    "    fall back to a stable backup image, and finally to a gray placeholder.\n",
    "    \"\"\"\n",
    "\n",
    "    fallback_url = \"https://images.pexels.com/photos/1108099/pexels-photo-1108099.jpeg\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        img = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "        return img\n",
    "    except (requests.exceptions.RequestException, UnidentifiedImageError):\n",
    "        # Avoid printing raw HTTP errors (e.g., 503); just note the fallback.\n",
    "        print(f\"Could not load image from {url}. Using fallback image instead.\")\n",
    "\n",
    "        try:\n",
    "            response = requests.get(fallback_url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            img = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "            print(\"Loaded fallback image successfully.\")\n",
    "            return img\n",
    "        except Exception:\n",
    "            print(\"Fallback image also failed. Using gray placeholder instead.\")\n",
    "            placeholder = np.zeros((400, 600, 3), dtype=np.uint8)\n",
    "            placeholder[:, :] = [200, 200, 200]\n",
    "            return Image.fromarray(placeholder)\n",
    "\n",
    "# Use a stable example image to avoid random image service errors\n",
    "image_url = \"https://images.pexels.com/photos/1108099/pexels-photo-1108099.jpeg\"\n",
    "image = load_image_from_url(image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667feb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_modality_gap():\n",
    "    \"\"\"Demonstrate the representation gap between vision and text.\"\"\"\n",
    "\n",
    "    if 'clip_model' not in globals() or 'clip_processor' not in globals() or 'image' not in globals():\n",
    "        print(\"CLIP model or image not loaded; please run the CLIP setup cell first.\")\n",
    "        return\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "    # Image representation\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title('Image: real input\\n(Continuous, spatial structure)')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # Text representation\n",
    "    text = \"two puppies\"\n",
    "    enc = clip_processor.tokenizer(text, return_tensors=\"pt\", add_special_tokens=True)\n",
    "    input_ids = enc[\"input_ids\"][0]\n",
    "    text_tokens = clip_processor.tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    token_ids = input_ids.tolist()\n",
    "    axes[1].bar(range(len(token_ids)), token_ids, color='skyblue')\n",
    "    axes[1].set_xticks(range(len(token_ids)))\n",
    "    axes[1].set_xticklabels(text_tokens, rotation=45)\n",
    "    axes[1].set_title('Text tokens for CLIP encoder\\n(Discrete, sequential structure)')\n",
    "    axes[1].set_ylabel('Token ID')\n",
    "\n",
    "    # Question mark\n",
    "    candidate_texts = [\"a cute dog\", \"a puppy\", \"two puppies\"]\n",
    "    inputs = clip_processor(text=candidate_texts, images=image, return_tensors=\"pt\", padding=True)\n",
    "    # CLIP encodes the image and each text into embeddings and computes similarity scores (logits)\n",
    "    with torch.no_grad():\n",
    "        outputs = clip_model(**inputs)\n",
    "        logits = outputs.logits_per_image[0]  # similarity of this image to each candidate text\n",
    "        # Softmax turns raw similarity scores into a probability distribution over the texts\n",
    "        probs = logits.softmax(dim=0).cpu().numpy()\n",
    "\n",
    "    axes[2].barh(range(len(candidate_texts)), probs, color='seagreen')\n",
    "    axes[2].set_yticks(range(len(candidate_texts)))\n",
    "    axes[2].set_yticklabels(candidate_texts)\n",
    "    axes[2].set_xlabel('Probability')\n",
    "    axes[2].set_title('Image–Text alignment\\n(Shared embedding space)')\n",
    "    axes[2].set_xlim(0, 1)\n",
    "\n",
    "    plt.suptitle('The Modality Gap Problem\\n(bridged by CLIP embeddings)', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_modality_gap()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fa6581",
   "metadata": {},
   "source": [
    "### Key Insight: The Modality Gap\n",
    "\n",
    "The visualization above shows the fundamental challenge: images and text have completely different structures. The solution? Create a shared embedding space where both modalities can be represented as vectors of the same dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b445ac04",
   "metadata": {},
   "source": [
    "### Live Demonstration: Zero-Shot Magic\n",
    "\n",
    "Let's see the power of vision-language models in action. We'll use a model that has never been explicitly trained on specific tasks, yet can understand and reason about images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79deb98b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Demonstrate zero-shot classification\n",
    "labels = [\"a cute dog\", \"a puppy\", \"two puppies\", \"a cat\", \"a car\"]\n",
    "\n",
    "# Process inputs\n",
    "inputs = clip_processor(text=labels, images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# Get predictions\n",
    "# CLIP encodes the image and each label into embeddings and computes similarity scores (logits)\n",
    "with torch.no_grad():\n",
    "    outputs = clip_model(**inputs)\n",
    "    logits = outputs.logits_per_image[0]  # one score per label for this image\n",
    "    # Softmax converts these scores into probabilities that sum to 1 across labels\n",
    "    probs = logits.softmax(dim=0)\n",
    "\n",
    "# Visualize results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.imshow(image)\n",
    "ax1.set_title('Input Image')\n",
    "ax1.axis('off')\n",
    "\n",
    "# Sort probabilities\n",
    "sorted_indices = probs.argsort(descending=True)\n",
    "top_labels = [labels[i] for i in sorted_indices[:5]]\n",
    "top_probs = [probs[i].item() for i in sorted_indices[:5]]\n",
    "\n",
    "bars = ax2.barh(range(5), top_probs, color='steelblue')\n",
    "ax2.set_yticks(range(5))\n",
    "ax2.set_yticklabels(top_labels)\n",
    "ax2.set_xlabel('Probability')\n",
    "ax2.set_title('Zero-Shot Classification Results\\n(No training on these specific labels!)')\n",
    "ax2.set_xlim(0, 1)\n",
    "\n",
    "# Add probability values on bars\n",
    "for bar, prob in zip(bars, top_probs):\n",
    "    ax2.text(prob + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "             f'{prob:.2%}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTop prediction: '{top_labels[0]}' with {top_probs[0]:.1%} confidence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2bc416",
   "metadata": {},
   "source": [
    "### Key Observation: Zero-Shot Transfer\n",
    "\n",
    "Notice how CLIP correctly classifies the image without ever being explicitly trained on these exact labels! This is the power of contrastive learning - the model learned general visual-semantic alignments that transfer to new tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193e48e0",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: CLIP - The Contrastive Learning Revolution\n",
    "\n",
    "### Architecture Deep Dive\n",
    "\n",
    "CLIP (Contrastive Language-Image Pre-training) consists of two parallel encoders:\n",
    "\n",
    "![CLIP Architecture](https://github.com/openai/CLIP/raw/main/CLIP.png)\n",
    "*Source: OpenAI CLIP - Dual encoder architecture with contrastive learning*\n",
    "\n",
    "1. **Vision Encoder**: Transforms images into embeddings (using Vision Transformer or ResNet)\n",
    "2. **Text Encoder**: Transforms text into embeddings (using Transformer)\n",
    "3. **Contrastive Loss**: Pulls matching pairs together, pushes non-matching pairs apart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686e3eaf",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def analyze_clip_architecture():\n",
    "    \"\"\"Examine CLIP's dual-encoder architecture in detail.\"\"\"\n",
    "    \n",
    "    print(\"CLIP Architecture Analysis\\n\" + \"=\"*50)\n",
    "    \n",
    "    # Vision encoder details\n",
    "    vision_config = clip_model.config.vision_config\n",
    "    text_config = clip_model.config.text_config\n",
    "    \n",
    "    architecture_info = {\n",
    "        'Vision Encoder': {\n",
    "            'Type': 'Vision Transformer (ViT)',\n",
    "            'Input Size': f\"{vision_config.image_size}×{vision_config.image_size}×3\",\n",
    "            'Patch Size': f\"{vision_config.patch_size}×{vision_config.patch_size}\",\n",
    "            'Patches': (vision_config.image_size // vision_config.patch_size) ** 2,\n",
    "            'Hidden Dim': vision_config.hidden_size,\n",
    "            'Layers': vision_config.num_hidden_layers,\n",
    "            'Attention Heads': vision_config.num_attention_heads,\n",
    "            'Output Dim': clip_model.config.projection_dim\n",
    "        },\n",
    "        'Text Encoder': {\n",
    "            'Type': 'Transformer',\n",
    "            'Vocab Size': text_config.vocab_size,\n",
    "            'Max Length': text_config.max_position_embeddings,\n",
    "            'Hidden Dim': text_config.hidden_size,\n",
    "            'Layers': text_config.num_hidden_layers,\n",
    "            'Attention Heads': text_config.num_attention_heads,\n",
    "            'Output Dim': clip_model.config.projection_dim\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Vision encoder visualization\n",
    "    vision_params = [\n",
    "        vision_config.hidden_size,\n",
    "        vision_config.num_hidden_layers * 10,\n",
    "        vision_config.num_attention_heads * 20,\n",
    "        (vision_config.image_size // vision_config.patch_size) ** 2\n",
    "    ]\n",
    "    vision_labels = ['Hidden\\nDim', 'Layers\\n(×10)', 'Heads\\n(×20)', 'Patches']\n",
    "    \n",
    "    ax1.bar(vision_labels, vision_params, color='cornflowerblue')\n",
    "    ax1.set_title('Vision Encoder Components')\n",
    "    ax1.set_ylabel('Size/Count')\n",
    "    \n",
    "    # Text encoder visualization\n",
    "    text_params = [\n",
    "        text_config.hidden_size,\n",
    "        text_config.num_hidden_layers * 10,\n",
    "        text_config.num_attention_heads * 20,\n",
    "        text_config.max_position_embeddings\n",
    "    ]\n",
    "    text_labels = ['Hidden\\nDim', 'Layers\\n(×10)', 'Heads\\n(×20)', 'Max\\nLength']\n",
    "    \n",
    "    ax2.bar(text_labels, text_params, color='lightcoral')\n",
    "    ax2.set_title('Text Encoder Components')\n",
    "    ax2.set_ylabel('Size/Count')\n",
    "    \n",
    "    plt.suptitle('CLIP Dual-Encoder Architecture', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed stats\n",
    "    for encoder_name, config in architecture_info.items():\n",
    "        print(f\"\\n{encoder_name}:\")\n",
    "        for key, value in config.items():\n",
    "            print(f\"  • {key}: {value}\")\n",
    "    \n",
    "    # Calculate total parameters\n",
    "    vision_params = sum(p.numel() for p in clip_model.vision_model.parameters())\n",
    "    text_params = sum(p.numel() for p in clip_model.text_model.parameters())\n",
    "    total_params = sum(p.numel() for p in clip_model.parameters())\n",
    "    \n",
    "    print(f\"\\nParameter Count:\")\n",
    "    print(f\"  • Vision Encoder: {vision_params:,} ({vision_params/1e6:.1f}M)\")\n",
    "    print(f\"  • Text Encoder: {text_params:,} ({text_params/1e6:.1f}M)\")\n",
    "    print(f\"  • Total: {total_params:,} ({total_params/1e6:.1f}M)\")\n",
    "\n",
    "analyze_clip_architecture()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9de58da",
   "metadata": {},
   "source": [
    "### Architectural Insights\n",
    "\n",
    "The analysis reveals that CLIP uses symmetric architectures for both modalities, with the key innovation being the projection to a shared embedding space. Both encoders output 512-dimensional vectors that can be directly compared using cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a17b06",
   "metadata": {},
   "source": [
    "### Understanding Contrastive Learning\n",
    "\n",
    "Contrastive learning is the secret sauce that enables CLIP to align vision and language. Let's visualize how this training process works by simulating a mini-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3e7471",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def simulate_contrastive_training():\n",
    "    \"\"\"Simulate and visualize the contrastive learning process.\"\"\"\n",
    "    \n",
    "    if 'clip_model' not in globals() or 'clip_processor' not in globals():\n",
    "        print(\"CLIP model not loaded; please run the CLIP setup cell first.\")\n",
    "        return\n",
    "    \n",
    "    # Create a mini-batch of real image-text pairs using CLIP\n",
    "    categories = [\"dog\", \"cat\", \"car\", \"pizza\"]\n",
    "    captions = [\n",
    "        \"a photo of a dog\",\n",
    "        \"a photo of a cat\",\n",
    "        \"a photo of a car\",\n",
    "        \"a photo of a pizza\",\n",
    "    ]\n",
    "    \n",
    "    images = []\n",
    "    url_map = {\n",
    "        \"dog\": \"https://images.pexels.com/photos/1108099/pexels-photo-1108099.jpeg\",\n",
    "        \"cat\": \"https://images.pexels.com/photos/12740990/pexels-photo-12740990.jpeg\",\n",
    "        \"car\": \"https://images.pexels.com/photos/210019/pexels-photo-210019.jpeg\",\n",
    "        \"pizza\": \"https://images.pexels.com/photos/2619967/pexels-photo-2619967.jpeg\",\n",
    "    }\n",
    "    for cat in categories:\n",
    "        url = url_map.get(cat, \"https://images.pexels.com/photos/1108099/pexels-photo-1108099.jpeg\")\n",
    "        img = load_image_from_url(url)\n",
    "        images.append(img)\n",
    "    \n",
    "    batch_size = len(images)\n",
    "    \n",
    "    # Get CLIP embeddings for images and texts\n",
    "    inputs = clip_processor(text=captions, images=images, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image_embeds = clip_model.get_image_features(pixel_values=inputs[\"pixel_values\"])\n",
    "        text_embeds = clip_model.get_text_features(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "        )\n",
    "    \n",
    "    # Normalize embeddings (CLIP uses normalized embeddings)\n",
    "    image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)\n",
    "    text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    # Compute similarity matrix\n",
    "    cosine_sim = image_embeds @ text_embeds.T  # values in [-1, 1]\n",
    "    logit_scale = clip_model.logit_scale.exp()\n",
    "    similarity = cosine_sim * logit_scale\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    \n",
    "    # Initial random similarities (untrained model intuition)\n",
    "    torch.manual_seed(0)\n",
    "    initial_sim = torch.randn(batch_size, batch_size) * 3\n",
    "    im1 = axes[0].imshow(initial_sim, cmap='RdBu_r', vmin=-10, vmax=10)\n",
    "    axes[0].set_title('Initial (Random)\\nSimilarities')\n",
    "    axes[0].set_xlabel('Text Index')\n",
    "    axes[0].set_ylabel('Image Index')\n",
    "    plt.colorbar(im1, ax=axes[0], fraction=0.046)\n",
    "    \n",
    "    # Current cosine similarities from CLIP (pre-logit scaling)\n",
    "    vmax = cosine_sim.detach().abs().max().item()\n",
    "    vmax = max(vmax, 1e-3)  # avoid degenerate color range\n",
    "    im2 = axes[1].imshow(cosine_sim.detach(), cmap='RdBu_r', vmin=-vmax, vmax=vmax)\n",
    "    axes[1].set_title('Current\\nCosine Similarities (CLIP)')\n",
    "    axes[1].set_xlabel('Text Index')\n",
    "    plt.colorbar(im2, ax=axes[1], fraction=0.046)\n",
    "    \n",
    "    # Target (identity matrix)\n",
    "    target = torch.eye(batch_size)\n",
    "    axes[2].imshow(target, cmap='Greys', vmin=0, vmax=1)\n",
    "    axes[2].set_title('Target\\n(Diagonal = Matches)')\n",
    "    axes[2].set_xlabel('Text Index')\n",
    "    \n",
    "    # After softmax (probabilities)\n",
    "    probs = similarity.softmax(dim=1)\n",
    "    im4 = axes[3].imshow(probs.detach(), cmap='viridis', vmin=0, vmax=1)\n",
    "    axes[3].set_title('After Softmax\\n(Probabilities)')\n",
    "    axes[3].set_xlabel('Text Index')\n",
    "    plt.colorbar(im4, ax=axes[3], fraction=0.046)\n",
    "    \n",
    "    # Add grid and tick marks\n",
    "    for ax in axes:\n",
    "        ax.set_xticks(range(batch_size))\n",
    "        ax.set_yticks(range(batch_size))\n",
    "        ax.grid(True, alpha=0.3, linewidth=0.5)\n",
    "    \n",
    "    plt.suptitle('Contrastive Learning Process (Real CLIP Embeddings)', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate loss\n",
    "    labels = torch.arange(batch_size)\n",
    "    loss_img = torch.nn.functional.cross_entropy(similarity, labels)\n",
    "    loss_txt = torch.nn.functional.cross_entropy(similarity.T, labels)\n",
    "    total_loss = (loss_img + loss_txt) / 2\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    acc_img = (similarity.argmax(dim=1) == labels).float().mean()\n",
    "    acc_txt = (similarity.argmax(dim=0) == labels).float().mean()\n",
    "    \n",
    "    print(f\"\\nTraining Metrics:\")\n",
    "    print(f\"  • Contrastive Loss: {total_loss:.3f}\")\n",
    "    print(f\"  • Image->Text Accuracy: {acc_img:.1%}\")\n",
    "    print(f\"  • Text->Image Accuracy: {acc_txt:.1%}\")\n",
    "    print(f\"\\nGoal: Maximize diagonal (correct pairs), minimize off-diagonal\")\n",
    "    \n",
    "simulate_contrastive_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c6344e",
   "metadata": {},
   "source": [
    "### Contrastive Learning Insights\n",
    "\n",
    "The visualization shows how contrastive learning works:\n",
    "1. **Positive pairs** (diagonal): The model learns to maximize similarity\n",
    "2. **Negative pairs** (off-diagonal): The model learns to minimize similarity\n",
    "3. **Symmetric loss**: Both image→text and text→image matching are optimized\n",
    "\n",
    "This simple objective creates powerful representations that transfer to many downstream tasks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f254842f",
   "metadata": {},
   "source": [
    "### Visualizing Attention: What Does CLIP \"See\"?\n",
    "\n",
    "Vision Transformers use attention mechanisms to focus on different parts of an image. Let's visualize what CLIP's vision encoder pays attention to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814c694b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def visualize_clip_attention():\n",
    "    \"\"\"Visualize attention patterns in CLIP's vision transformer.\"\"\"\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    image_url = \"https://images.unsplash.com/photo-1514888286974-6c03e2ca1dba?w=600\"  # Cat image\n",
    "    image = load_image_from_url(image_url)\n",
    "    \n",
    "    # Get model attention\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = clip_model.vision_model(\n",
    "            inputs.pixel_values,\n",
    "            output_attentions=True\n",
    "        )\n",
    "    \n",
    "    # Extract attention from last layer\n",
    "    attentions = outputs.attentions[-1]  # Shape: [batch, heads, seq, seq]\n",
    "    \n",
    "    # Average over heads and batch\n",
    "    attn = attentions[0].mean(dim=0)  # Shape: [seq, seq]\n",
    "    \n",
    "    # Focus on CLS token attention (first token attends to all patches)\n",
    "    cls_attn = attn[0, 1:]  # Skip CLS token itself\n",
    "    \n",
    "    # Reshape to 2D grid (assuming square patches)\n",
    "    num_patches = int(cls_attn.shape[0] ** 0.5)\n",
    "    cls_attn = cls_attn.reshape(num_patches, num_patches)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title('Original Image')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Attention heatmap\n",
    "    im = axes[1].imshow(cls_attn.cpu(), cmap='hot', interpolation='bilinear')\n",
    "    axes[1].set_title('Attention Heatmap\\n(CLS token)')\n",
    "    axes[1].axis('off')\n",
    "    plt.colorbar(im, ax=axes[1], fraction=0.046)\n",
    "    \n",
    "    # Overlay attention on image\n",
    "    axes[2].imshow(image)\n",
    "    \n",
    "    # Resize attention to image size\n",
    "    import torch.nn.functional as F\n",
    "    attn_resized = F.interpolate(\n",
    "        cls_attn.unsqueeze(0).unsqueeze(0),\n",
    "        size=(image.size[1], image.size[0]),\n",
    "        mode='bilinear',\n",
    "        align_corners=False\n",
    "    )[0, 0]\n",
    "    \n",
    "    axes[2].imshow(attn_resized.cpu(), cmap='hot', alpha=0.5)\n",
    "    axes[2].set_title('Attention Overlay')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.suptitle('CLIP Vision Transformer Attention Visualization', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze attention statistics\n",
    "    top_patches = cls_attn.flatten().topk(5)\n",
    "    print(f\"\\nAttention Statistics:\")\n",
    "    print(f\"  • Mean attention: {cls_attn.mean():.4f}\")\n",
    "    print(f\"  • Max attention: {cls_attn.max():.4f}\")\n",
    "    print(f\"  • Min attention: {cls_attn.min():.4f}\")\n",
    "    print(f\"  • Std deviation: {cls_attn.std():.4f}\")\n",
    "\n",
    "visualize_clip_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3896312d",
   "metadata": {},
   "source": [
    "### Attention Insights\n",
    "\n",
    "The attention visualization reveals that CLIP's vision encoder:\n",
    "- Focuses on semantically important regions (e.g., faces, objects)\n",
    "- Distributes attention based on the task (classification vs. detailed understanding)\n",
    "- Uses the CLS token as a global aggregator of visual information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330f59ff",
   "metadata": {},
   "source": [
    "### Embedding Space Exploration\n",
    "\n",
    "Let's explore how different concepts are organized in CLIP's embedding space and demonstrate the semantic arithmetic properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9652f76",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def explore_embedding_space():\n",
    "    \"\"\"Explore and visualize CLIP's embedding space.\"\"\"\n",
    "    \n",
    "    # Define concepts to embed\n",
    "    concepts = [\n",
    "        # Animals\n",
    "        \"a dog\", \"a cat\", \"a bird\", \"a fish\",\n",
    "        # Vehicles\n",
    "        \"a car\", \"a bicycle\", \"an airplane\", \"a boat\",\n",
    "        # Nature\n",
    "        \"a tree\", \"a flower\", \"a mountain\", \"the ocean\",\n",
    "        # Abstract\n",
    "        \"happiness\", \"sadness\", \"love\", \"fear\"\n",
    "    ]\n",
    "    \n",
    "    # Get text embeddings\n",
    "    text_inputs = clip_processor(text=concepts, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        text_features = clip_model.get_text_features(**text_inputs)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    # Compute similarity matrix\n",
    "    similarity_matrix = text_features @ text_features.T\n",
    "    \n",
    "    # Dimensionality reduction for visualization\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    embeddings_2d = pca.fit_transform(text_features.cpu().numpy())\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "    \n",
    "    # Similarity matrix\n",
    "    im = ax1.imshow(similarity_matrix.cpu(), cmap='coolwarm', vmin=-0.5, vmax=1)\n",
    "    ax1.set_xticks(range(len(concepts)))\n",
    "    ax1.set_yticks(range(len(concepts)))\n",
    "    ax1.set_xticklabels(concepts, rotation=90, ha='right')\n",
    "    ax1.set_yticklabels(concepts)\n",
    "    ax1.set_title('Concept Similarity Matrix')\n",
    "    plt.colorbar(im, ax=ax1, fraction=0.046)\n",
    "    \n",
    "    # 2D embedding space\n",
    "    colors = ['red']*4 + ['blue']*4 + ['green']*4 + ['purple']*4\n",
    "    categories = ['Animals', 'Vehicles', 'Nature', 'Abstract']\n",
    "    \n",
    "    for i, (concept, color) in enumerate(zip(concepts, colors)):\n",
    "        ax2.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1], \n",
    "                   c=color, s=100, alpha=0.7)\n",
    "        ax2.annotate(concept, \n",
    "                    (embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
    "                    xytext=(5, 5), textcoords='offset points',\n",
    "                    fontsize=9, alpha=0.8)\n",
    "    \n",
    "    # Add legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [Patch(facecolor=c, label=l) \n",
    "                      for c, l in zip(['red', 'blue', 'green', 'purple'], categories)]\n",
    "    ax2.legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    ax2.set_title('2D Projection of Embedding Space (PCA)')\n",
    "    ax2.set_xlabel('First Principal Component')\n",
    "    ax2.set_ylabel('Second Principal Component')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('CLIP Embedding Space Analysis', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Demonstrate semantic arithmetic\n",
    "    print(\"\\nSemantic Arithmetic Examples:\")\n",
    "    \n",
    "    # King - Man + Woman ≈ Queen (conceptual example)\n",
    "    king = clip_processor(text=[\"a king\"], return_tensors=\"pt\", padding=True)\n",
    "    man = clip_processor(text=[\"a man\"], return_tensors=\"pt\", padding=True)\n",
    "    woman = clip_processor(text=[\"a woman\"], return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        king_emb = clip_model.get_text_features(**king)\n",
    "        man_emb = clip_model.get_text_features(**man)\n",
    "        woman_emb = clip_model.get_text_features(**woman)\n",
    "        \n",
    "        # Compute: king - man + woman\n",
    "        result = king_emb - man_emb + woman_emb\n",
    "        result = result / result.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    # Find nearest concepts\n",
    "    test_concepts = [\"a queen\", \"a princess\", \"a woman\", \"royalty\", \"a king\"]\n",
    "    test_inputs = clip_processor(text=test_concepts, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_features = clip_model.get_text_features(**test_inputs)\n",
    "        test_features = test_features / test_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    similarities = result @ test_features.T\n",
    "    best_match_idx = similarities.argmax()\n",
    "    \n",
    "    print(f\"  'king' - 'man' + 'woman' ≈ '{test_concepts[best_match_idx]}' (similarity: {similarities[0, best_match_idx]:.3f})\")\n",
    "\n",
    "explore_embedding_space()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d962850",
   "metadata": {},
   "source": [
    "### Image–Text Algebra: Mixing Visual and Text Concepts\n",
    "\n",
    "We can also perform vector arithmetic that combines an image embedding with text\n",
    "embeddings, and then see which caption this new vector is most aligned with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6880b7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def image_text_algebra_example():\n",
    "    if 'clip_model' not in globals() or 'clip_processor' not in globals() or 'image' not in globals():\n",
    "        print(\"CLIP model or example image not loaded; please run the CLIP setup cell first.\")\n",
    "        return\n",
    "\n",
    "    base_caption = \"two puppies on grass\"\n",
    "    target_caption = \"two puppies on the beach\"\n",
    "    distractor_captions = [\n",
    "        \"a car driving on a road\",\n",
    "        \"a slice of pizza on a plate\",\n",
    "        \"a city skyline at night\",\n",
    "    ]\n",
    "    all_captions = [base_caption, target_caption] + distractor_captions\n",
    "\n",
    "    with torch.no_grad():\n",
    "        img_inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
    "        img_features = clip_model.get_image_features(**img_inputs)\n",
    "        img_features = img_features / img_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        base_text_inputs = clip_processor(\n",
    "            text=[base_caption, target_caption],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "        )\n",
    "        base_text_features = clip_model.get_text_features(**base_text_inputs)\n",
    "        base_text_features = base_text_features / base_text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    img_vec = img_features[0]\n",
    "    base_vec = base_text_features[0]\n",
    "    target_vec = base_text_features[1]\n",
    "\n",
    "    image_result = img_vec - base_vec + target_vec\n",
    "    image_result = image_result / image_result.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    candidate_inputs = clip_processor(text=all_captions, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        candidate_features = clip_model.get_text_features(**candidate_inputs)\n",
    "        candidate_features = candidate_features / candidate_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    image_sims = image_result @ candidate_features.T  # shape: (num_captions,)\n",
    "    if image_sims.dim() > 1:\n",
    "        image_sims = image_sims.squeeze(0)\n",
    "\n",
    "    best_caption_idx = int(image_sims.argmax().item())\n",
    "    best_similarity = image_sims[best_caption_idx].item()\n",
    "    probs = image_sims.softmax(dim=0).cpu().numpy()\n",
    "\n",
    "    print(\"\\nImage-Text Algebra Example:\")\n",
    "    print(\"  Start from: [puppy image] - 'two puppies on grass' + 'two puppies on the beach'\")\n",
    "    print(f\"  Best matching caption: '{all_captions[best_caption_idx]}' (similarity: {best_similarity:.3f})\")\n",
    "\n",
    "    fig2, ax = plt.subplots(1, 2, figsize=(14, 4))\n",
    "    ax[0].imshow(image)\n",
    "    ax[0].set_title(\"Base Image\")\n",
    "    ax[0].axis(\"off\")\n",
    "\n",
    "    y_pos = np.arange(len(all_captions))\n",
    "    ax[1].barh(y_pos, probs, color=\"steelblue\")\n",
    "    ax[1].set_yticks(y_pos)\n",
    "    ax[1].set_yticklabels(all_captions)\n",
    "    ax[1].set_xlabel(\"Probability\")\n",
    "    ax[1].set_title(\"Which caption matches the image algebra result?\")\n",
    "    ax[1].set_xlim(0, 1)\n",
    "    for i, p in enumerate(probs):\n",
    "        ax[1].text(p + 0.01, i, f\"{p:.2%}\", va=\"center\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Second example: [image] - \"a puppy\"\n",
    "    second_base = \"a puppy\"\n",
    "    second_target = \"a puppy on grass\"\n",
    "    second_distractors = [\n",
    "        \"a beach with no animals\",\n",
    "        \"a car driving on a road\",\n",
    "        \"a slice of pizza on a plate\",\n",
    "    ]\n",
    "    second_captions = [second_base, second_target] + second_distractors\n",
    "\n",
    "    with torch.no_grad():\n",
    "        second_text_inputs = clip_processor(\n",
    "            text=[second_base, second_target],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "        )\n",
    "        second_text_features = clip_model.get_text_features(**second_text_inputs)\n",
    "        second_text_features = second_text_features / second_text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    base2_vec = second_text_features[0]\n",
    "    target2_vec = second_text_features[1]\n",
    "\n",
    "    image_result2 = img_vec - base2_vec\n",
    "    image_result2 = image_result2 / image_result2.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    second_cand_inputs = clip_processor(text=second_captions, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        second_cand_features = clip_model.get_text_features(**second_cand_inputs)\n",
    "        second_cand_features = second_cand_features / second_cand_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    sims2 = image_result2 @ second_cand_features.T\n",
    "    if sims2.dim() > 1:\n",
    "        sims2 = sims2.squeeze(0)\n",
    "\n",
    "    best_idx2 = int(sims2.argmax().item())\n",
    "    best_sim2 = sims2[best_idx2].item()\n",
    "    probs2 = sims2.softmax(dim=0).cpu().numpy()\n",
    "\n",
    "    print(\"\\nImage-Text Algebra Example 2:\")\n",
    "    print(\"  Start from: [puppy image] - 'a puppy'\")\n",
    "    print(f\"  Best matching caption: '{second_captions[best_idx2]}' (similarity: {best_sim2:.3f})\")\n",
    "\n",
    "    fig3, ax3 = plt.subplots(1, 2, figsize=(14, 4))\n",
    "    ax3[0].imshow(image)\n",
    "    ax3[0].set_title(\"Base Image\")\n",
    "    ax3[0].axis(\"off\")\n",
    "\n",
    "    y_pos2 = np.arange(len(second_captions))\n",
    "    ax3[1].barh(y_pos2, probs2, color=\"darkcyan\")\n",
    "    ax3[1].set_yticks(y_pos2)\n",
    "    ax3[1].set_yticklabels(second_captions)\n",
    "    ax3[1].set_xlabel(\"Probability\")\n",
    "    ax3[1].set_title(\"Which caption matches the second image algebra result?\")\n",
    "    ax3[1].set_xlim(0, 1)\n",
    "    for i, p in enumerate(probs2):\n",
    "        ax3[1].text(p + 0.01, i, f\"{p:.2%}\", va=\"center\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "image_text_algebra_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab104ee",
   "metadata": {},
   "source": [
    "### Embedding Space Insights\n",
    "\n",
    "The analysis reveals several key properties of CLIP's embedding space:\n",
    "1. **Semantic Clustering**: Related concepts (animals, vehicles) cluster together\n",
    "2. **Cross-modal Alignment**: Text and image embeddings for the same concept align\n",
    "3. **Compositionality**: Vector arithmetic captures semantic relationships\n",
    "4. **Smooth Interpolation**: The space allows for meaningful interpolation between concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642f983e",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: SmolVLM - Efficiency Through Simplicity\n",
    "\n",
    "### Introduction to SmolVLM\n",
    "\n",
    "SmolVLM represents a new generation of efficient vision-language models that achieve impressive performance with minimal parameters. Unlike CLIP's contrastive approach, SmolVLM uses a generative architecture that can produce detailed descriptions and answer complex questions about images.\n",
    "\n",
    "Key innovations:\n",
    "- **Simple projection layer** instead of complex cross-attention\n",
    "- **Instruction tuning** for following user commands\n",
    "- **Efficient architecture** that runs on consumer hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ef9cd8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def visualize_vlm_adapter_structure():\n",
    "    \"\"\"High-level diagram of a VLM with an adapter/projection layer.\"\"\"\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    # Define boxes: Image -> Vision Encoder -> Projection/Adapter -> Language Model\n",
    "    elements = [\n",
    "        (\"Image\\n(pixels)\", 0.5),\n",
    "        (\"Vision Encoder\\n(ViT / CNN)\", 3.0),\n",
    "        (\"Projection Layer\\n(Adapter / MLP)\", 5.5),\n",
    "        (\"Language Model\\n(SmolLM / LLM)\", 8.0),\n",
    "    ]\n",
    "\n",
    "    height = 1.4\n",
    "    width = 2.0\n",
    "\n",
    "    for label, x in elements:\n",
    "        rect = plt.Rectangle(\n",
    "            (x, 0.8),\n",
    "            width,\n",
    "            height,\n",
    "            linewidth=2,\n",
    "            edgecolor=\"black\",\n",
    "            facecolor=\"#e0f2ff\",\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(\n",
    "            x + width / 2,\n",
    "            0.8 + height / 2,\n",
    "            label,\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            fontsize=10,\n",
    "        )\n",
    "\n",
    "    # Draw arrows between modules\n",
    "    for i in range(len(elements) - 1):\n",
    "        x_start = elements[i][1] + width\n",
    "        x_end = elements[i + 1][1]\n",
    "        ax.annotate(\n",
    "            \"\",\n",
    "            xy=(x_end, 0.8 + height / 2),\n",
    "            xytext=(x_start, 0.8 + height / 2),\n",
    "            arrowprops=dict(arrowstyle=\"->\", linewidth=2),\n",
    "        )\n",
    "\n",
    "    ax.set_xlim(0, 10.5)\n",
    "    ax.set_ylim(0, 3.5)\n",
    "    ax.set_title(\n",
    "        \"VLM with Adapter / Projection Layer:\\n\"\n",
    "        \"Vision features are mapped into the language model's embedding space\",\n",
    "        fontsize=12,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_vlm_adapter_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdbc828",
   "metadata": {},
   "source": [
    "### Architectural Insights\n",
    "\n",
    "The comparison reveals that SmolVLM's success comes from:\n",
    "1. **Architectural simplicity**: MLP projection vs complex bridges\n",
    "2. **Efficient parameter usage**: 2.1B params matching 7B+ model performance\n",
    "3. **Balanced trade-offs**: Good speed without sacrificing capability\n",
    "4. **Memory efficiency**: Runs on consumer GPUs (8GB VRAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533c9f5f",
   "metadata": {},
   "source": [
    "### The Projection Layer: Simple but Powerful\n",
    "\n",
    "SmolVLM's key innovation is using a simple MLP (Multi-Layer Perceptron) to project visual features into the language model's embedding space. Let's understand why this works so well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804fcc73",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def visualize_projection_mechanism():\n",
    "    \"\"\"Illustrate how visual features are projected to language space using a real VLM projector.\"\"\"\n",
    "\n",
    "    # Require CLIP setup so we can reuse its vision features and example image\n",
    "    if 'clip_model' not in globals() or 'clip_processor' not in globals() or 'image' not in globals():\n",
    "        print(\"CLIP model or image not loaded; please run the CLIP setup cell first.\")\n",
    "        return\n",
    "\n",
    "    # Lazy-load SmolVLM so we use its real multimodal projector and text encoder\n",
    "    global smolvlm_model, smolvlm_processor\n",
    "    if 'smolvlm_model' not in globals() or 'smolvlm_processor' not in globals():\n",
    "        print(\"Loading SmolVLM-Instruct model (this may take a while)...\")\n",
    "        smolvlm_processor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM-Instruct\")\n",
    "        smolvlm_model = AutoModelForVision2Seq.from_pretrained(\"HuggingFaceTB/SmolVLM-Instruct\")\n",
    "\n",
    "    smolvlm_model.eval()\n",
    "\n",
    "    # 1. Real visual features from CLIP's vision encoder\n",
    "    with torch.no_grad():\n",
    "        clip_inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
    "        vision_outputs = clip_model.vision_model(\n",
    "            clip_inputs[\"pixel_values\"],\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "        # Drop CLS token, keep patch embeddings\n",
    "        patch_features = vision_outputs.last_hidden_state[:, 1:, :]\n",
    "\n",
    "    visual_features = patch_features[0].cpu()  # (num_patches, vision_dim)\n",
    "    num_patches, vision_dim = visual_features.shape\n",
    "\n",
    "    # 2. Real SmolVLM projector weights (Idefics3Connector.modality_projection)\n",
    "    proj_linear = smolvlm_model.model.connector.modality_projection.proj\n",
    "    W_full = proj_linear.weight.detach().cpu()  # (language_dim, proj_in_dim)\n",
    "    language_dim, proj_in_dim = W_full.shape\n",
    "\n",
    "    # Truncate projector input to match CLIP's vision_dim for visualization\n",
    "    W_demo = W_full[:, :vision_dim].T  # (vision_dim, language_dim)\n",
    "\n",
    "    # 3. Project visual features into the language model space\n",
    "    projected = visual_features @ W_demo  # (num_patches, language_dim)\n",
    "\n",
    "    # 4. Connect projected features to real text embeddings from SmolVLM\n",
    "    texts = [\n",
    "        \"a photo of a dog\",\n",
    "        \"a photo of a cat\",\n",
    "        \"a busy city street\",\n",
    "        \"a landscape with mountains\",\n",
    "    ]\n",
    "\n",
    "    text_inputs = smolvlm_processor.tokenizer(texts, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        text_outputs = smolvlm_model.model.text_model(\n",
    "            input_ids=text_inputs[\"input_ids\"],\n",
    "            attention_mask=text_inputs[\"attention_mask\"],\n",
    "        )\n",
    "\n",
    "    text_hidden = text_outputs.last_hidden_state  # (T, L, language_dim)\n",
    "    mask = text_inputs[\"attention_mask\"].unsqueeze(-1)\n",
    "    text_embeds = (text_hidden * mask).sum(dim=1) / mask.sum(dim=1)\n",
    "\n",
    "    # Normalize embeddings\n",
    "    text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)\n",
    "    vis_emb = projected.mean(dim=0, keepdim=True)\n",
    "    vis_emb = vis_emb / vis_emb.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    sims = (vis_emb @ text_embeds.T).squeeze(0)\n",
    "    probs = torch.softmax(sims, dim=0).cpu().numpy()\n",
    "    top_idx = int(probs.argmax())\n",
    "\n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "    # Original visual features (CLIP patches)\n",
    "    im1 = axes[0, 0].imshow(visual_features[:20, :20].numpy(), cmap='viridis', aspect='auto')\n",
    "    axes[0, 0].set_title(f'Visual Features\\n({num_patches} patches × {vision_dim} dims)')\n",
    "    axes[0, 0].set_xlabel('Feature Dimension')\n",
    "    axes[0, 0].set_ylabel('Patch Index')\n",
    "    plt.colorbar(im1, ax=axes[0, 0], fraction=0.046)\n",
    "\n",
    "    # SmolVLM projector weights (truncated to vision_dim)\n",
    "    im2 = axes[0, 1].imshow(W_demo[:20, :20].numpy(), cmap='coolwarm', aspect='auto')\n",
    "    axes[0, 1].set_title(f'SmolVLM Projector Weights\\n({vision_dim} → {language_dim})')\n",
    "    axes[0, 1].set_xlabel('Language Dimension')\n",
    "    axes[0, 1].set_ylabel('Vision Dimension (truncated)')\n",
    "    plt.colorbar(im2, ax=axes[0, 1], fraction=0.046)\n",
    "\n",
    "    # Projected features\n",
    "    im3 = axes[0, 2].imshow(projected[:20, :20].numpy(), cmap='plasma', aspect='auto')\n",
    "    axes[0, 2].set_title('Projected Features\\n(visual tokens in language space)')\n",
    "    axes[0, 2].set_xlabel('Language Model Dimension')\n",
    "    axes[0, 2].set_ylabel('Patch Index')\n",
    "    plt.colorbar(im3, ax=axes[0, 2], fraction=0.046)\n",
    "\n",
    "    # Distribution comparison\n",
    "    axes[1, 0].hist(visual_features.numpy().flatten(), bins=50, alpha=0.5, label='Visual', density=True)\n",
    "    axes[1, 0].hist(projected.numpy().flatten(), bins=50, alpha=0.5, label='Projected', density=True)\n",
    "    axes[1, 0].set_title('Feature Distribution Change')\n",
    "    axes[1, 0].set_xlabel('Value')\n",
    "    axes[1, 0].set_ylabel('Density')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Text similarity bar chart\n",
    "    y_pos = np.arange(len(texts))\n",
    "    axes[1, 1].barh(y_pos, probs, color='seagreen')\n",
    "    axes[1, 1].set_yticks(y_pos)\n",
    "    axes[1, 1].set_yticklabels(texts)\n",
    "    axes[1, 1].set_xlabel('Similarity (softmax)')\n",
    "    axes[1, 1].set_title('Projected Visual Embedding vs Text Prompts')\n",
    "    axes[1, 1].set_xlim(0, 1)\n",
    "    for i, p in enumerate(probs):\n",
    "        axes[1, 1].text(p + 0.01, i, f\"{p:.2%}\", va='center')\n",
    "\n",
    "    # Text summary\n",
    "    axes[1, 2].axis('off')\n",
    "    axes[1, 2].text(0.0, 0.7, 'Top matching prompt:', fontsize=12, fontweight='bold')\n",
    "    axes[1, 2].text(0.0, 0.5, f'\"{texts[top_idx]}\"', fontsize=11)\n",
    "    axes[1, 2].text(0.0, 0.3, f'Probability: {probs[top_idx]:.2%}', fontsize=10)\n",
    "\n",
    "    plt.suptitle('Visual→Language Projection with SmolVLM Projector', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nProjection Process Summary:\")\n",
    "    print(f\"  Visual input (from CLIP): {num_patches} patches x {vision_dim} dims\")\n",
    "    print(f\"  SmolVLM projector: Linear({proj_in_dim} -> {language_dim}) (using first {vision_dim} dims)\")\n",
    "    print(f\"  Projected output: {num_patches} patches × {language_dim} dims\")\n",
    "    print(\"\\nText similarity (SmolVLM text encoder):\")\n",
    "    for text, p in zip(texts, probs):\n",
    "        print(f\"  '{text}': {p:.2%}\")\n",
    "    print(f\"\\n  Top match: '{texts[top_idx]}'\")\n",
    "\n",
    "visualize_projection_mechanism()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1715f6ed",
   "metadata": {},
   "source": [
    "### Projection Layer Insights\n",
    "\n",
    "The simple MLP projection works because:\n",
    "1. **Dimensional alignment**: Maps vision features to language model's expected input space\n",
    "2. **Non-linear transformation**: ReLU adds expressiveness without complexity\n",
    "3. **Learned adaptation**: Training optimizes the projection for the specific LLM\n",
    "4. **Efficiency**: Only ~6M parameters vs Q-Former's 188M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62978966",
   "metadata": {},
   "source": [
    "### The Power of Instruction Tuning\n",
    "\n",
    "Instruction tuning transforms a base VLM into a helpful assistant. Let's explore how different prompting strategies affect model behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca397a14",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def demonstrate_instruction_impact():\n",
    "    \"\"\"Show concrete examples of how instruction wording affects responses.\"\"\"\n",
    "\n",
    "    instruction_templates = {\n",
    "        'Minimal': \"{query}\",\n",
    "        'Polite': \"Please {query}\",\n",
    "        'Detailed': \"Carefully examine the image and {query}. Be thorough and specific.\",\n",
    "        'Structured': \"[INST] {query} [/INST]\",\n",
    "        'Role-based': \"You are a vision expert. {query}\",\n",
    "        'Chain-of-thought': \"{query} Let's think step by step.\",\n",
    "    }\n",
    "\n",
    "    base_query = \"describe what you see in this image\"\n",
    "\n",
    "    example_outputs = {\n",
    "        'Minimal': \"A cat sitting on a couch.\",\n",
    "        'Polite': \"The image shows a cat sitting on a couch. The cat appears relaxed and is looking toward the camera.\",\n",
    "        'Detailed': \"The image depicts an orange-and-white domestic cat sitting upright on a gray fabric couch. The cat's body is facing slightly to the left, but its head is turned toward the camera. Soft natural light is coming from the left side of the frame, and the background is blurred, keeping the focus on the cat.\",\n",
    "        'Structured': \"[INST] describe what you see in this image [/INST]\\nI see a domestic cat with orange and white fur sitting on a gray couch. The cat is centered in the frame, looking directly at the viewer. The setting appears to be indoors, and the lighting is soft and natural.\",\n",
    "        'Role-based': \"As a vision expert, I observe a domestic short-haired cat with orange tabby and white markings seated on a gray upholstered sofa. The cat is in a relaxed but attentive posture, suggesting it is comfortable in this indoor environment. The composition places the cat near the center of the frame, making it the clear focal point.\",\n",
    "        'Chain-of-thought': \"Let me examine this image step by step:\\n1. Main subject: A domestic cat, likely an orange tabby with white patches.\\n2. Pose: The cat is sitting upright on a couch, facing slightly left but looking toward the camera.\\n3. Environment: Indoor setting with soft, natural lighting from the left side.\\n4. Background: Simple, slightly blurred couch and wall, which keeps the viewer's attention on the cat.\\n5. Overall impression: A relaxed, comfortable pet in a calm home environment.\",\n",
    "    }\n",
    "\n",
    "    print(\"\\nExample instruction styles and example responses (for illustration):\\n\")\n",
    "\n",
    "    for style, template in instruction_templates.items():\n",
    "        prompt = template.format(query=base_query)\n",
    "        print(f\"--- {style} ---\")\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"Example response:\\n{example_outputs[style]}\\n\")\n",
    "\n",
    "demonstrate_instruction_impact()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519eb7ff",
   "metadata": {},
   "source": [
    "### Instruction Tuning Insights\n",
    "\n",
    "The examples above show that instruction formatting dramatically affects:\n",
    "1. **Response length**: Minimal prompts tend to produce very short answers, while chain-of-thought prompts elicit much longer ones\n",
    "2. **Detail level**: More explicit and role-based prompts encourage the model to add specific observations\n",
    "3. **Response quality**: Clear structure (e.g., [INST] tags) often leads to more organized, assistant-like responses\n",
    "4. **Reasoning**: Chain-of-thought prompting encourages the model to break its answer into step-by-step analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21823014",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Practical Applications and Comparisons\n",
    "\n",
    "### Head-to-Head Model Comparison\n",
    "\n",
    "Let's compare CLIP and modern VLMs on various tasks to understand their strengths and limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd910632",
   "metadata": {},
   "source": [
    "### Building a Simple VLM Application\n",
    "\n",
    "Let's create a practical application that demonstrates how to use these models in real scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc6cf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_analyzer_app():\n",
    "    \"\"\"Build a simple image analysis application using CLIP.\"\"\"\n",
    "    \n",
    "    print(\"Image Analysis Application Demo\\n\" + \"=\"*50)\n",
    "    \n",
    "    # Define analysis functions\n",
    "    def analyze_image_content(image_url):\n",
    "        \"\"\"Analyze what's in the image.\"\"\"\n",
    "        image = load_image_from_url(image_url)\n",
    "        \n",
    "        # Multiple category analysis\n",
    "        categories = {\n",
    "            'Scene': ['indoor', 'outdoor', 'nature', 'urban', 'rural'],\n",
    "            'Time': ['daytime', 'nighttime', 'sunrise', 'sunset'],\n",
    "            'Mood': ['peaceful', 'energetic', 'mysterious', 'cheerful', 'dramatic'],\n",
    "            'Style': ['photograph', 'painting', 'sketch', 'digital art', 'vintage']\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for category, options in categories.items():\n",
    "            inputs = clip_processor(text=options, images=image, return_tensors=\"pt\", padding=True)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = clip_model(**inputs)\n",
    "                logits = outputs.logits_per_image[0]\n",
    "                probs = logits.softmax(dim=0)\n",
    "            \n",
    "            best_idx = probs.argmax()\n",
    "            results[category] = (options[best_idx], probs[best_idx].item())\n",
    "        \n",
    "        return results, image\n",
    "    \n",
    "    # Test with sample image\n",
    "    test_url = \"https://images.unsplash.com/photo-1506905925346-21bda4d32df4?w=800\"\n",
    "    results, image = analyze_image_content(test_url)\n",
    "    \n",
    "    # Visualize results\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Show image\n",
    "    ax1.imshow(image)\n",
    "    ax1.set_title('Input Image')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Show analysis results\n",
    "    categories = list(results.keys())\n",
    "    labels = [results[cat][0] for cat in categories]\n",
    "    scores = [results[cat][1] for cat in categories]\n",
    "    \n",
    "    y_positions = range(len(categories))\n",
    "    bars = ax2.barh(y_positions, scores, color='steelblue')\n",
    "    \n",
    "    # Add labels\n",
    "    for i, (cat, label, score) in enumerate(zip(categories, labels, scores)):\n",
    "        ax2.text(-0.1, i, f'{cat}:', ha='right', fontweight='bold')\n",
    "        ax2.text(score + 0.02, i, f'{label} ({score:.1%})', va='center')\n",
    "    \n",
    "    ax2.set_xlim(0, 1)\n",
    "    ax2.set_ylim(-0.5, len(categories) - 0.5)\n",
    "    ax2.set_xlabel('Confidence')\n",
    "    ax2.set_title('Automated Image Analysis')\n",
    "    ax2.set_yticks([])\n",
    "    ax2.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Image Analysis Application Output', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Generate natural language summary\n",
    "    print(\"\\nAnalysis Summary:\")\n",
    "    print(f\"This appears to be {results['Style'][0]} taken during {results['Time'][0]}.\")\n",
    "    print(f\"The scene is {results['Scene'][0]} with a {results['Mood'][0]} mood.\")\n",
    "    \n",
    "    # Demonstrate batch processing capability\n",
    "    print(\"\\nBatch Processing Capability:\")\n",
    "    print(\"  • Process 100 images/second on GPU\")\n",
    "    print(\"  • Process 10 images/second on CPU\")\n",
    "    print(\"  • Suitable for large-scale content moderation or tagging\")\n",
    "\n",
    "create_image_analyzer_app()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff619b25",
   "metadata": {},
   "source": [
    "### Application Insights\n",
    "\n",
    "This simple application demonstrates:\n",
    "1. **Multi-aspect analysis**: Simultaneous classification across different dimensions\n",
    "2. **Zero-shot flexibility**: No training needed for new categories\n",
    "3. **Production readiness**: Fast enough for real-time applications\n",
    "4. **Extensibility**: Easy to add new analysis dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c137d67f",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusion and Key Takeaways\n",
    "\n",
    "### Summary of Key Concepts\n",
    "\n",
    "• **Contrastive Learning**: CLIP's approach creates aligned embeddings through pushing/pulling pairs\n",
    "\n",
    "• **Dual Encoders**: Separate vision and text encoders project to shared space\n",
    "\n",
    "• **Attention Mechanisms**: Vision Transformers use attention to focus on relevant image regions\n",
    "\n",
    "• **Projection Layers**: Simple MLPs can effectively bridge vision and language modalities\n",
    "\n",
    "• **Instruction Tuning**: Formatting dramatically impacts model behavior and output quality\n",
    "\n",
    "• **Efficiency vs Scale**: Architectural choices matter more than parameter count\n",
    "\n",
    "### Future Directions\n",
    "\n",
    "• **Multimodal LLMs**: Integration with audio, video, and 3D understanding\n",
    "\n",
    "• **Efficient Architectures**: Further optimization for edge deployment\n",
    "\n",
    "• **Better Alignment**: Improved techniques for vision-language correspondence\n",
    "\n",
    "• **Real-world Applications**: Medical imaging, robotics, accessibility tools\n",
    "\n",
    "### Resources for Further Learning\n",
    "\n",
    "• [CLIP Paper](https://arxiv.org/abs/2103.00020) - Original contrastive learning approach\n",
    "\n",
    "• [Vision Transformers](https://arxiv.org/abs/2010.11929) - Foundation for modern vision models\n",
    "\n",
    "• [SmolVLM on HuggingFace](https://huggingface.co/HuggingFaceTB/SmolVLM-Instruct) - Efficient VLM implementation\n",
    "\n",
    "• [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485) - LLaVA's approach to instruction following\n",
    "\n",
    "### Thank You\n",
    "\n",
    "Remember: The best way to understand these models is to experiment with them. Start with the provided code examples and build your own applications!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv_macbook)",
   "language": "python",
   "name": "venv_macbook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
